\subsubsection{Lecture 7 (27. Januar 2025)}

\paragraph{Goal for today}
\begin{itemize}
    \item Newton's Method:
    \begin{itemize}
        \item Definition and convergence rate
        \item Combination with line search
        \item Necessary modifications
    \end{itemize}
\end{itemize}

\section*{Newton's Method}

\paragraph{Background}
Assume \(G: \R^d \to \R^d\). We want to solve \(G(x) = 0\).

Newton's method is an iterative procedure for finding the roots of a differentiable function \(G: \R^d \to \R^d\). 
It uses the first-order Taylor (linear) approximation of \(G\) at the current point \(x_k\).

\begin{definition}{Newton's Method}{}
    The iteration is defined by
    \begin{align*}
        x_{k+1} &= x_k - \bigl(D G(x_k)\bigr)^{-1} G(x_k),
    \end{align*}
    where \(D G(x_k)\) is the derivative (Jacobian) of \(G\) at \(x_k\). If \(G\) is the gradient of a scalar function \(f\), then \(DG(x_k) = H_f(x_k)\), the Hessian of \(f\).
\end{definition}

\paragraph{Convergence Theory}
If \(G\in \mathcal{C}^2\), then this iteration converges locally \emph{quadratically} to 
a solution \(x^\star\) of \(G(x) = 0\), provided that \(D G(x^\star)\) is invertible.

\begin{remark}
    Quadratic convergence means there exist constants \(C>0\) and \(\delta>0\) such that for all \(k\) with \(\|x_k - x^\star\|\le \delta\),
    \[
        \|x_{k+1} - x^\star\| \;\le\; C \,\|x_k - x^\star\|^2.
    \]
\end{remark}

\subsection*{Application to Optimization}

We apply Newton's method to solve \(\nabla f(x) = 0\). That is, to find stationary points of a function \(f:\R^d \to \R\).

\paragraph{Basic Method}
\begin{itemize}
    \item Newton iteration:
    \[
      x_{k+1} \;=\; x_k \;-\; H_f(x_k)^{-1} \,\nabla f(x_k).
    \]
    \item Alternatively, set up the system \(H_f(x_k)\,p_k = -\,\nabla f(x_k)\) and then \(x_{k+1} = x_k + p_k\).
    \item Convergence: If \(f\in \mathcal{C}^3\) and \(H_f(x^\star)\) is nonsingular at the local minimizer \(x^\star\), the method converges locally quadratically.
\end{itemize}

\begin{algorithm}[H]
\caption{Basic Newton's Method}
\label{alg:newton-basic}
\KwInput{Starting point $x_0$, tolerance $\epsilon > 0$}
\KwOutput{Approximate solution $x_k$}
$k \gets 0$\;
\While{$\|\nabla f(x_k)\| > \epsilon$}{
    Compute Hessian $H_f(x_k)$\;
    Solve $H_f(x_k)\,p_k = -\,\nabla f(x_k)$\;
    $x_{k+1} \gets x_k + p_k$\;
    $k \gets k + 1$\;
}
\KwRet{$x_k$}\;
\end{algorithm}

\paragraph{Global Convergence Strategy}
\begin{enumerate}
    \item Combine with a line search.
    \item Define \(p_k\) by solving \(H_f(x_k)\,p_k = -\nabla f(x_k)\).
    \item Choose a step length \(\alpha_k > 0\).
    \item Update \(x_{k+1} = x_k + \alpha_k \, p_k\).
\end{enumerate}

\begin{algorithm}[H]
\caption{Globally Convergent Newton's Method}
\label{alg:newton-global}
\KwInput{Starting point $x_0$, tolerances $\epsilon, c > 0$, reduction factor $\rho \in (0,1)$}
\KwOutput{Approximate solution $x_k$}
$k \gets 0$\;
\While{$\|\nabla f(x_k)\| > \epsilon$}{
    Compute Hessian $H_f(x_k)$\;
    Solve $H_f(x_k)\,p_k = -\nabla f(x_k)$\;
    $\alpha_k \gets 1$\;
    \While{$f(x_k + \alpha_k p_k) > f(x_k) + c\,\alpha_k\,\nabla f(x_k)^\top p_k$}{
        $\alpha_k \gets \rho\,\alpha_k$\;
    }
    $x_{k+1} \gets x_k + \alpha_k p_k$\;
    $k \gets k + 1$\;
}
\KwRet{$x_k$}\;
\end{algorithm}

\paragraph{Descent Direction Condition}
If \(H_f(x_k)\) is positive definite, then
\[
    \langle p_k, \nabla f(x_k)\rangle
    \;=\;
    -\,\langle H_f(x_k)^{-1}\,\nabla f(x_k),\,\nabla f(x_k)\rangle
    \;<\; 0,
\]
so \(p_k\) is a descent direction for the line search.

\paragraph{Modified Newton's Method}
If \(H_f(x_k)\) is not positive definite, or if \(\langle \nabla f(x_k), p_k\rangle \geq 0\), then the line search may fail. One approach is to modify the Hessian or switch to a different direction:

\begin{itemize}
    \item First compute \(p_k = -\,H_f(x_k)^{-1}\,\nabla f(x_k)\).
    \item If \(\langle \nabla f(x_k),\,p_k\rangle \;\ge\; \varepsilon \,\|\nabla f(x_k)\|\,\|p_k\|\) for some \(\varepsilon > 0\), switch to \(p_k = -\,\nabla f(x_k)\).
    \item Alternatively, modify the Hessian so it is sufficiently positive definite (e.g.\ shift its eigenvalues).
\end{itemize}

\begin{algorithm}[H]
\caption{Modified Newton's Method with Hessian Modification}
\label{alg:newton-modified}
\KwInput{Starting point $x_0$, tolerances $\epsilon, \varepsilon > 0$}
\KwOutput{Approximate solution $x_k$}
$k \gets 0$\;
\While{$\|\nabla f(x_k)\| > \epsilon$}{
    Compute Hessian $H_f(x_k)$\;
    Compute eigendecomposition $H_f(x_k) = U\,\Lambda\,U^\top$\;
    $M \gets \text{diag}\bigl(\max\{\lambda_1, \varepsilon\}, \ldots, \max\{\lambda_d, \varepsilon\}\bigr)$\;
    $p_k \gets -\,U\,M^{-1}\,U^\top\,\nabla f(x_k)$\;
    \uIf{$\nabla f(x_k)^\top p_k < -\,\varepsilon\,\|\nabla f(x_k)\|\|p_k\|$}{
        Use $p_k$ as the search direction\;
    }
    \Else{
        $p_k \gets -\,\nabla f(x_k)$\;
    }
    % In a global method, we would then do a line search on p_k:
    %   x_{k+1} = x_k + alpha_k p_k
    % but here it is omitted for brevity.
    $x_{k+1} \gets x_k + p_k$\;
    $k \gets k + 1$\;
}
\KwRet{$x_k$}\;
\end{algorithm}

\paragraph{Alternative Interpretation of Newton's Method}
Newton's method can also be viewed via a second-order Taylor expansion of \(f\) around \(x_k\):

\begin{enumerate}
    \item Approximate \(f\) by 
    \[
        m_k(p) \;=\; f(x_k) \;+\; \langle \nabla f(x_k),\, p\rangle \;+\; \tfrac12\,\langle H_f(x_k)\,p,\,p\rangle.
    \]
    \item Find \(p_k\) by minimizing \(m_k(p)\). 
    \[
      \nabla m_k(p_k) \;=\; \nabla f(x_k) \;+\; H_f(x_k)\,p_k \;=\; 0
      \quad\Longrightarrow\quad
      p_k = -\,H_f(x_k)^{-1}\,\nabla f(x_k).
    \]
    \item If \(H_f(x_k)\) is not positive definite, one modifies it to be positive definite (e.g.\ shifting eigenvalues).
\end{enumerate}

The \emph{modified Newton's method} then reads \(x_{k+1} = x_k + \alpha_k\,p_k\) with \(\alpha_k>0\) found by a line search and \(p_k\) given by one of the strategies above.

\paragraph{Quadratic Convergence Criterion}
Quadratic convergence holds precisely when, near \(x^\star\), we use
\[
    p_k \;=\; -\,H_f(x_k)^{-1}\,\nabla f(x_k)
    \quad\text{and}\quad
    \alpha_k \;=\; 1.
\]
In particular, for \(\alpha_k = 1\) to be acceptable in the line search once \(x_k\) is close to \(x^\star\), the Hessian near \(x^\star\) must be uniformly positive definite (so all eigenvalues \(\ge \varepsilon > 0\)), and the line search parameters (e.g.\ the Armijo or Wolfe constants) must allow a full step.

\section*{Conjugate Gradient Methods}

We now focus on the special case of minimizing the quadratic function
\[
\Phi(x) \;:=\; \tfrac12\,\langle x,\;Q\,x\rangle \;-\; \langle b,\;x\rangle,
\]
where \(Q \in \R^{d\times d}\) is symmetric positive definite and \(b \in \R^d\). In exact arithmetic, the \emph{Conjugate Gradient} (CG) method can solve this problem in at most \(d\) steps.

With exact line search, the update is
\[
x_{k+1} 
\;=\; x_k \;-\; \frac{\langle Q\,x_k - b,\;p_k\rangle}{\langle Q\,p_k,\;p_k\rangle}\,p_k,
\]
which ensures 
\[
\langle \nabla \Phi(x_{k+1}),\;p_k\rangle \;=\; 0.
\]
In particular, for a gradient method we would have \(\nabla \Phi(x_{k+1}) \propto p_{k+1}\). Orthogonality conditions arise naturally here.

\paragraph{Conjugate Directions}
A set of vectors \(\{p_0, p_1, \dots, p_{d-1}\}\subset \R^d\) is \emph{conjugate} with respect to \(Q\) if
\[
    \langle p_i,\;Q\,p_j\rangle \;=\; 0 \quad\text{for}\quad i\neq j.
\]
Equivalently, the vectors are mutually orthogonal under the inner product \(\langle u, v \rangle_Q := \langle u, Q\,v\rangle\).

\begin{lemma}{Conjugate Directions}{}
    Let \(\{p_0,\ldots,p_{d-1}\}\) be a conjugate basis of \(\R^d\). Consider the iteration
    \[
    x_{k+1} \;=\; x_k + \alpha_k\,p_k
    \quad\text{with}\quad
    \alpha_k \;=\; -\,\frac{\langle r_k,\;p_k\rangle}{\langle Q\,p_k,\;p_k\rangle},
    \]
    where \(r_k = Q\,x_k - b\). Then \(x_d = x^\star = Q^{-1}b\). In other words, the exact solution is reached in at most \(d\) steps.
\end{lemma}

\begin{proof}{}{}
Since \(\{p_0,\ldots,p_{d-1}\}\) is a basis, there exist scalars \(\sigma_k\) such that
\[
x^\star - x_0 
\;=\; \sum_{k=0}^{d-1} \sigma_k\,p_k.
\]
Also,
\[
x_d - x_0 
\;=\; \sum_{k=0}^{d-1} \alpha_k\,p_k.
\]
To show \(x_d = x^\star\), it suffices to prove \(\sigma_k = \alpha_k\) for all \(k\). Indeed,
\[
    \sigma_l 
    \;=\; \frac{\langle x^\star - x_0,\;Q\,p_l\rangle}{\langle p_l,\;Q\,p_l\rangle}
    \;=\; \frac{\langle Q\,(x^\star - x_0),\;p_l\rangle}{\langle p_l,\;Q\,p_l\rangle}
    \;=\; \frac{\langle b - Q\,x_0,\;p_l\rangle}{\langle p_l,\;Q\,p_l\rangle}.
\]
Also,
\[
    \alpha_l
    \;=\; -\,\frac{\langle r_0,\;p_l\rangle}{\langle Q\,p_l,\;p_l\rangle},
    \quad\text{where}\quad r_0 \;=\; Q\,x_0 - b.
\]
Careful manipulation (and using the fact that \(\langle p_i, Q\,p_j\rangle=0\) for \(i\neq j\)) shows \(\sigma_l = \alpha_l\). Hence \(x^\star = x_d\).
\end{proof}

\paragraph{The CG Iteration}
The Conjugate Gradient method constructs a sequence of conjugate directions by choosing
\[
p_0 = -\,r_0 \;=\; b - Q\,x_0,
\quad
p_{k+1} = -\,r_{k+1} + \beta_k\,p_k
\]
with \(\beta_k\) chosen so that \(\langle p_{k+1},\,Q\,p_k\rangle=0\). One can show this leads to convergence in at most \(d\) steps for solving \(Q\,x = b\) in exact arithmetic.

\end{document}
