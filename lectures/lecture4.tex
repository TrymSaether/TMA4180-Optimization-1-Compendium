\subsubsection{Lecture 4 (17. Januar 2025)}

\subsubsection*{Numerical methods for free optimization}

\textbf{Goal:}

We want to solve \( \min_{x \in \R^d} f(x) \) numerically. Want to find an approximate solution to the problem with:

\begin{itemize}
    \item Good accuracy
    \item sufficiently fast
\end{itemize}

\textbf{Assumption:} Given $ x \in \R^d $, we have a black-box that computes $f(x), \nabla f(x), \nabla^2 f(x) , \ldots $.

\textbf{Can expect:}
\begin{itemize}
    \item calculation of \(f(x)\) is much more expensive than additions/multiplications/divisions of \(x\).
    \item calculation of \(\nabla f(x)\) is more expensive than \(f(x)\).
\end{itemize}

It should try to minimize the number of function and gradient evaluations.

\subsection*{Nelder-Mead Algorithm}
\textbf{Properties:}
\begin{itemize}
    \item Derivative-free method
    \item Only require $f(x)$, but not $\nabla f(x)$.
          \begin{itemize}
              \item $\nabla f(x)$ might be to expensive to compute.
              \item $\nabla f(x)$ might be unavailable.
              \item $f(x)$ might not be differentiable.
          \end{itemize}
    \item Works well for low-dimensional problems, but not for high-dimensional problems.
\end{itemize}

\textbf{Idea of Nelder-Mead:}
Choose $d+1$ affinely independent points in $\R^d$.
\begin{align*}
    x_1, x_2, \ldots, x_{d+1} \in \R^d \\
    f(x_1) \leq f(x_2) \leq \ldots \leq f(x_{d+1})
\end{align*}
Then replace the worst point $x_{d+1}$ with a better point:

define $\bar{x} := \frac{1}{d} \sum_{k=0}^{d-1} x_k$ and choose a new point on the line $\bar{x} - t(x_{d} - \bar{x})$ with $t < 1$.

Take particularly: $t= -1$, $t = -2$, $t = 0.5$ or $t = 0.5$.

\begin{itemize}
    \item \textit{If:} one of these points $f(x_k)$ is better than $f(x_{d})$, replace $x_{d}$ by this point.
    \item \textit{Else:} replace $x_k$ by $\frac{1}{2}(x_k + x_0)$ for $k = 0, 1, \ldots, d$.
\end{itemize}

\textbf{Theory:} almost non-existent.
\begin{itemize}
    \item If $f \in \mathcal{C}^1(\R^2)$ is strictly convex, then Nelder-Mead converges or more specific the diameter $d(x_1, x_{d+1})$ converges to zero (but the limit point is not necessarily a minimizer).
\end{itemize}

\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Initial simplex with $d+1$ points}
\KwOut{Optimal point}
Initialize simplex with $d+1$ points\;
\While{convergence criteria not met}{
    Order points by function values\;
    Compute centroid of best $d$ points\;
    Reflect worst point through centroid\;
    \eIf{reflected point better than second worst but not better than best}{
        Accept reflection\;
    }{
        \eIf{reflected point is best}{
            Try expanding\;
        }{
            \eIf{reflected point worse than second worst}{
                Perform contraction\;
            }{
                Shrink simplex towards best point\;
            }
        }
    }
}
\Return{best point}\;
\caption{Nelder-Mead Algorithm}
\end{algorithm}

\subsection*{Gradient Descent and backtracking}
\vspace{-0.1cm}
\textit{Line search methods}

\subsubsection*{Gradient Descent idea}
\begin{enumerate}
    \item Start at a point \( x_0 \).
    \item Compute the gradient \( \nabla f(x_0) \).
    \item Move in the direction of the negative gradient.
    \item Update the point: \( x_1 = x_0 - \alpha \nabla f(x_0) \).
    \item Repeat until convergence.
\end{enumerate}

since:
\begin{align*}
    f(x_k + \alpha p_k) = f(x_k) + \alpha \ip{\nabla f(x_k), p_k} + o(\alpha) \\
    \rightarrow \ip{\nabla f(x_k), p_k} < 0 \quad \text{for} \quad \alpha > 0 \quad \text{small enough}.
\end{align*}

\begin{example}{}{}
    If $p_k = - \nabla f(x_k) \rightarrow \text{(GD)}$.

    Newtons method: $p_k = H_f(x_k)^{-1} \nabla f(x_k)$ if $H_f(x_k)$ is positive definite.
\end{example}

\subsubsection*{Algorithm}
\begin{algorithm}[H]
\SetAlgoLined
\KwIn{Initial point $x_0$, tolerance $\epsilon > 0$}
\KwOut{Local minimum $x^*$}
$k \gets 0$\;
\While{$\|\nabla f(x_k)\| \geq \epsilon$}{
    Compute $\nabla f(x_k)$\;
    Choose step size $\alpha_k$\;
    $x_{k+1} \gets x_k - \alpha_k \nabla f(x_k)$\;
    $k \gets k + 1$\;
}
\Return{$x_k$}\;
\caption{Gradient Descent}
\end{algorithm}

\begin{remark}{Step length selection}{}
    Theoretical idea is exact line search. Choose $\alpha_k > 0$ s.t. $f(x_k + \alpha_k p_k)$ is minimal.
    $\alpha_k$ solves $\min_{\alpha > 0} f(x_k + \alpha p_k)$.
\end{remark}

\begin{definition}{Armijo condition}{}
    Choose $\alpha_k$ s.t. $f(x_k + \alpha_k p_k) \leq f(x_k) + c_1 \alpha_k \ip{\nabla f(x_k), p_k}$.
\end{definition}

In practice: try different step lengths $\alpha_k$ until the Armijo condition is satisfied.

First idea: require that $f(x_k + \alpha_k p_k) \leq f(x_k)$ $\rightsquigarrow$ does not guarantee convergence.

\begin{lemma}{}{}
    Assume that $f \in \mathcal{C}^1(\R^d)$ is bounded below, that $p_k = - \nabla f(x_k) \neq 0$ and that the Armijo condition is satisfied for each step, then:

    \[
        \sum_{k=0}^{\infty} \alpha_k \norm{\nabla f(x_k)}^2 < \infty
    \]

    In particular if $0 < c \leq \alpha_k \; \forall k \rightarrow \nabla f(x_k) \rightarrow 0$.
\end{lemma}

