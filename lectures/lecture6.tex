
\subsubsection{Lecture 6 (24. Januar 2025)}

\section*{Backtracking Linjesøk}
Backtracking Line Search er en metode for å finne en passende skrittlengde \( \alpha \) i iterative optimeringsalgoritmer. Målet er å sikre at skrittet reduserer objektivfunksjonen \( f(\bm{x} + \alpha \bm{p}) \) tilstrekkelig, samtidig som det unngås for små eller ustabile skritt.

\subsection*{Algoritme og matematisk formulering}
La \( f: \R^n \to \R \) være kontinuerlig deriverbar, og \( \bm{p} \in \R^n \) en nedstigningsretning (dvs. \( \ip{\nabla f(\bm{x})}{\bm{p}} < 0 \)). Algoritmen søker \( \alpha > 0 \) som tilfredsstiller \textbf{Armijo-betingelsen}:
\begin{equation}
    f(\bm{x} + \alpha \bm{p}) \leq f(\bm{x}) + c_1 \alpha \ip{\nabla f(\bm{x})}{\bm{p}}, \quad c_1 \in (0, 1).
\end{equation}

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \KwIn{$\alpha_0 > 0$, $c_1 \in (0,1)$, $\rho \in (0,1)$}
    \KwOut{$\alpha$}
    $\alpha \leftarrow \alpha_0$\;
    \While{$f(\bm{x} + \alpha \bm{p}) > f(\bm{x}) + c_1 \alpha \ip{\nabla f(\bm{x})}{\bm{p}}$}{
        $\alpha \leftarrow \rho \alpha$ \tcp*{Reduser skrittlengde}
    }
    \Return{$\alpha$}
    \caption{Backtracking Line Search (Armijo)}
\end{algorithm}

\subsection*{Relaterte betingelser}
\begin{itemize}
    \item \textbf{Armijo-Goldstein}: Legger til nedre grense for \( \alpha \):
          \begin{equation}
              f(\bm{x}) + c_2 \alpha \ip{\nabla f(\bm{x})}{\bm{p}} \leq f(\bm{x} + \alpha \bm{p}) \leq f(\bm{x}) + c_1 \alpha \ip{\nabla f(\bm{x})}{\bm{p}}, \quad 0 < c_1 < c_2 < 1.
          \end{equation}

    \item \textbf{Wolfe-betingelsene}: Inkluderer krumningstilstand:
          \begin{equation}
              \ip{\nabla f(\bm{x} + \alpha \bm{p})}{\bm{p}} \geq c_2 \ip{\nabla f(\bm{x})}{\bm{p}}, \quad 0 < c_1 < c_2 < 1.
          \end{equation}
\end{itemize}

\begin{lemma}{Terminering}{}
    Hvis \( f \in C^2 \) er nedre begrenset og \( \bm{p} \) er en nedstigningsretning, terminerer backtracking-algoritmen med en endelig \( \alpha > 0 \).
\end{lemma}

\begin{theorem}{Konvergens for Gradient Descent}{}
    Anta:
    \begin{itemize}
        \item \( f \in C^2 \) med kompakt nivåmengde \( \mathcal{L}_f(\bm{x}_0) \)
        \item \( \bm{p}_k = -\nabla f(\bm{x}_k) \)
        \item \( \alpha_k \) tilfredsstiller Armijo/Wolfe
    \end{itemize}
    Da gjelder:
    \begin{equation}
        \lim_{k \to \infty} \norm{\nabla f(\bm{x}_k)} = 0
    \end{equation}
\end{theorem}

\begin{example}{Kvadratisk funksjon}{}
    For \( f(\bm{x}) = \frac{1}{2}\bm{x}^T Q \bm{x} - \bm{b}^T \bm{x} \) med \( Q \succ 0 \), gir eksakt linjesøk:
    \begin{align*}
        \alpha_k                             & = -\frac{\ip{\bm{p}_k}{\nabla f(\bm{x}_k)}}{\ip{\bm{p}_k}{Q \bm{p}_k}}, \\
        \norm{\bm{x}_{k+1} - \bm{x}^\star}_Q & \leq \frac{\kappa - 1}{\kappa + 1} \norm{\bm{x}_k - \bm{x}^\star}_Q,
    \end{align*}
    hvor \( \kappa = \cond(Q) = \frac{\lambda_{\max}(Q)}{\lambda_{\min}(Q)} \).
\end{example}

\begin{example}{Rosenbrock-funksjonen: Hessian}{}
    For \( f(x,y) = (1-x)^2 + 100(y-x^2)^2 \) har Hessianen i minimum \( \bm{x}^\star = (1,1) \) formen:
    \begin{equation}
        H_f(1,1) = \bmat{802 & -400 \\ -400 & 200}.
    \end{equation}
    Kondisjonstallet \( \kappa \approx 2500 \) forklarer treg konvergens for gradientdescent.
\end{example}