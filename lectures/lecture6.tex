\subsubsection{Lecture 6 (24. Januar 2025)}

\section*{Backtracking Linjesøk}
Backtracking Line Search er en metode for å finne en passende skrittlengde \( \alpha \) i iterative optimeringsalgoritmer. Målet er å sikre at skrittet reduserer objektivfunksjonen \( f(\bm{x} + \alpha \bm{p}) \) tilstrekkelig, samtidig som det unngås for små eller ustabile skritt.

\subsection*{Algoritme og Matematisk Formulering}
La \( f: \R^n \to \R \) være kontinuerlig deriverbar, og \( \bm{p} \in \R^n \) en nedstigningsretning (dvs. \( \langle \nabla f(\bm{x}), \bm{p} \rangle < 0 \)). Algoritmen søker \( \alpha > 0 \) som tilfredsstiller \textbf{Armijo-betingelsen}:
\begin{equation}
    f(\bm{x} + \alpha \bm{p}) \leq f(\bm{x}) + c_1 \alpha \langle \nabla f(\bm{x}), \bm{p} \rangle, \quad c_1 \in (0, 1).
\end{equation}

\begin{algorithm}[H]
    \SetAlgoLined
    \DontPrintSemicolon
    \KwIn{$\alpha_0 > 0$, $c_1 \in (0,1)$, $\rho \in (0,1)$}
    \KwOut{$\alpha$}
    $\alpha \leftarrow \alpha_0$\;
    \While{$f(\bm{x} + \alpha \bm{p}) > f(\bm{x}) + c_1 \alpha \langle \nabla f(\bm{x}), \bm{p} \rangle$}{
        $\alpha \leftarrow \rho \alpha$ \tcp*{Reduser skrittlengde}
    }
    \Return{$\alpha$}
    \caption{Backtracking Linjesøk (Armijo)}
\end{algorithm}

\subsection*{Relaterte Betingelser}
\begin{itemize}
    \item \textbf{Armijo-Goldstein}: Legger til en nedre grense for \( \alpha \):
    \begin{equation}
        f(\bm{x}) + c_2 \alpha \langle \nabla f(\bm{x}), \bm{p} \rangle \leq f(\bm{x} + \alpha \bm{p}) \leq f(\bm{x}) + c_1 \alpha \langle \nabla f(\bm{x}), \bm{p} \rangle, \quad 0 < c_1 < c_2 < 1.
    \end{equation}

    \item \textbf{Wolfe-betingelsene}: Inkluderer krumningstilstand:
    \begin{equation}
        \langle \nabla f(\bm{x} + \alpha \bm{p}), \bm{p} \rangle \geq c_2 \langle \nabla f(\bm{x}), \bm{p} \rangle, \quad 0 < c_1 < c_2 < 1.
    \end{equation}
\end{itemize}

\begin{lemma}{Terminering}{}
    Hvis \( f \in C^2 \) er nedre begrenset og \( \bm{p} \) er en nedstigningsretning, terminerer backtracking-algoritmen med en endelig \( \alpha > 0 \).
\end{lemma}

\begin{theorem}{Konvergens for Gradient Descent}{}
    Anta:
    \begin{itemize}
        \item \( f \in C^2 \) med kompakt nivåmengde \( \mathcal{L}_f(\bm{x}_0) = \{\bm{x} \in \R^n \mid f(\bm{x}) \leq f(\bm{x}_0)\} \)
        \item \( \bm{p}_k = -\nabla f(\bm{x}_k) \)
        \item \( \alpha_k \) tilfredsstiller Armijo- eller Wolfe-betingelsene
    \end{itemize}
    Da gjelder:
    \begin{equation}
        \lim_{k \to \infty} \|\nabla f(\bm{x}_k)\| = 0.
    \end{equation}
\end{theorem}

\begin{example}{Kvadratisk Funksjon}{}
    For \( f(\bm{x}) = \frac{1}{2} \bm{x}^\top Q \bm{x} - \bm{b}^\top \bm{x} \) med \( Q \succ 0 \), gir eksakt linjesøk:
    \begin{align*}
        \alpha_k                             & = -\frac{\langle \bm{p}_k, \nabla f(\bm{x}_k) \rangle}{\langle \bm{p}_k, Q \bm{p}_k \rangle}, \\
        \|\bm{x}_{k+1} - \bm{x}^\star\|_Q & \leq \frac{\kappa - 1}{\kappa + 1} \|\bm{x}_k - \bm{x}^\star\|_Q,
    \end{align*}
    hvor \( \kappa = \operatorname{cond}(Q) = \frac{\lambda_{\max}(Q)}{\lambda_{\min}(Q)} \).
\end{example}

\begin{example}{Rosenbrock-funksjonen: Hessian}{}
    For \( f(x,y) = (1 - x)^2 + 100(y - x^2)^2 \) har Hessianen i minimum \( \bm{x}^\star = (1,1) \) formen:
    \begin{equation}
        H_f(1,1) = \begin{bmatrix} 802 & -400 \\ -400 & 200 \end{bmatrix}.
    \end{equation}
    Kondisjonstallet \( \kappa \approx 2500 \) forklarer treg konvergens for gradientdescent.
\end{example}
