\part{General Constrained Optimization}

\chapter{Non-Convex Constrained Optimization}

\section{Tangent Cones for Nonconvex Sets}

For general (possibly nonconvex) sets, the tangent cone plays a crucial role in optimality conditions.

\begin{definition}{Tangent Cone}{tangent_cone}
    For a set $\Omega \subset \mathbb{R}^n$ and a point $x \in \Omega$, the tangent cone $T_\Omega(x)$ is defined as:
    \[
    T_\Omega(x) = \left\{ d \in \mathbb{R}^n : \exists \{x_k\} \subset \Omega, \{t_k\} \subset \mathbb{R}^+, t_k \downarrow 0, \lim_{k\to\infty} \frac{x_k - x}{t_k} = d \right\}
    \]
\end{definition}

The tangent cone generalizes the concept of feasible directions to nonconvex settings. For convex sets, the tangent cone coincides with the cone of feasible directions.

\section{Linear Independence Constraint Qualification (LICQ)}

In constrained optimization with both equality and inequality constraints:
\begin{mini*}
    {x}{f(x)}{}{}
    \addConstraint{g_i(x)}{\leq 0,\quad}{i \in \mathcal{I}}
    \addConstraint{h_j(x)}{= 0,\quad}{j \in \mathcal{E}}
\end{mini*}

we often encounter situations where certain qualification conditions must be satisfied to ensure well-behaved optimality conditions.

\begin{definition}{Active Set}{active_set}
    For a feasible point $x$, the active set $\mathcal{A}(x)$ is:
    \[
    \mathcal{A}(x) = \{i \in \mathcal{I} : g_i(x) = 0\} \cup \mathcal{E}
    \]
\end{definition}

\begin{definition}{Linear Independence Constraint Qualification (LICQ)}{licq}
    For a feasible point $x$, the LICQ is satisfied if the set of active constraint gradients:
    \[
    \{\nabla g_i(x) : i \in \mathcal{A}(x) \cap \mathcal{I}\} \cup \{\nabla h_j(x) : j \in \mathcal{E}\}
    \]
    is linearly independent.
\end{definition}

LICQ ensures that the constraints are well-posed and that Lagrange multipliers for the optimal solution are unique.

\section{KKT Conditions for Non-convex Problems}

\begin{theorem}{KKT Necessary Conditions}{kkt_necessary}
    Let $x^*$ be a local minimum of the constrained optimization problem, and suppose LICQ holds at $x^*$. Then there exist unique Lagrange multipliers $\lambda^* \in \mathbb{R}^{|\mathcal{I}|}$ and $\mu^* \in \mathbb{R}^{|\mathcal{E}|}$ such that:
    \begin{align}
        \nabla f(x^*) + \sum_{i \in \mathcal{I}} \lambda_i^* \nabla g_i(x^*) + \sum_{j \in \mathcal{E}} \mu_j^* \nabla h_j(x^*) &= 0 \tag{Stationarity}\\
        g_i(x^*) &\leq 0, \quad \forall i \in \mathcal{I} \tag{Primal Feasibility}\\
        h_j(x^*) &= 0, \quad \forall j \in \mathcal{E} \tag{Primal Feasibility}\\
        \lambda_i^* &\geq 0, \quad \forall i \in \mathcal{I} \tag{Dual Feasibility}\\
        \lambda_i^* g_i(x^*) &= 0, \quad \forall i \in \mathcal{I} \tag{Complementary Slackness}
    \end{align}
\end{theorem}

Unlike the convex case, these conditions are only necessary but not sufficient for optimality in nonconvex problems.

\section{Second-order Necessary and Sufficient Conditions}

For nonconvex problems, second-order conditions provide additional information about the nature of critical points.

\begin{definition}{Critical Cone}{critical_cone}
    For a point $x^*$ satisfying the KKT conditions with multipliers $(\lambda^*, \mu^*)$, the critical cone $\mathcal{C}(x^*, \lambda^*)$ is:
    \[
    \mathcal{C}(x^*, \lambda^*) = \{d \in T_\Omega(x^*) : \nabla g_i(x^*)^T d = 0 \text{ for all } i \in \mathcal{A}(x^*) \text{ with } \lambda_i^* > 0\}
    \]
\end{definition}

\begin{theorem}{Second-order Necessary Condition}{second_order_necessary}
    Let $x^*$ be a local minimum satisfying LICQ, with associated Lagrange multipliers $(\lambda^*, \mu^*)$. Then:
    \[
    d^T \nabla^2_{xx} \mathcal{L}(x^*, \lambda^*, \mu^*) d \geq 0 \quad \forall d \in \mathcal{C}(x^*, \lambda^*)
    \]
    where $\mathcal{L}$ is the Lagrangian function.
\end{theorem}

\begin{theorem}{Second-order Sufficient Condition}{second_order_sufficient}
    Let $x^*$ be a point satisfying the KKT conditions with multipliers $(\lambda^*, \mu^*)$. If:
    \[
    d^T \nabla^2_{xx} \mathcal{L}(x^*, \lambda^*, \mu^*) d > 0 \quad \forall d \in \mathcal{C}(x^*, \lambda^*) \setminus \{0\}
    \]
    then $x^*$ is a strict local minimum.
\end{theorem}

\chapter{Numerical Methods for Constrained Optimization}

\section{Penalty Methods}

Penalty methods transform constrained problems into unconstrained problems by adding a penalty term to the objective function.

\subsection{Quadratic Penalty Method}
\begin{mini*}
    {x}{f(x) + \frac{\mu}{2} \left( \sum_{i \in \mathcal{I}} [\max(0, g_i(x))]^2 + \sum_{j \in \mathcal{E}} [h_j(x)]^2 \right)}{}{}
\end{mini*}
where $\mu > 0$ is the penalty parameter.

\subsection{Exact Penalty Method}
\begin{mini*}
    {x}{f(x) + \mu \left( \sum_{i \in \mathcal{I}} \max(0, g_i(x)) + \sum_{j \in \mathcal{E}} |h_j(x)| \right)}{}{}
\end{mini*}

The key properties of penalty methods include:
\begin{itemize}
    \item Solutions of the penalized problem approach the solution of the original problem as $\mu \to \infty$
    \item Exact penalty methods can find the exact solution for finite $\mu$, but the objective becomes non-differentiable
    \item Quadratic penalties preserve differentiability but require $\mu \to \infty$ for exact solutions
\end{itemize}

\section{Logarithmic Barrier Methods}

For problems with inequality constraints only, barrier methods use a barrier function that prevents iterates from leaving the feasible region:

\begin{mini*}
    {x}{f(x) - \frac{1}{\mu} \sum_{i \in \mathcal{I}} \log(-g_i(x))}{}{}
\end{mini*}

The barrier term $-\log(-g_i(x))$ approaches infinity as $g_i(x)$ approaches zero, creating a barrier at the constraint boundary.

As $\mu \to \infty$, solutions of the barrier problem converge to the solution of the original problem.

\section{Augmented Lagrangian Methods}

Augmented Lagrangian methods combine penalty terms with Lagrangian terms:

\begin{mini*}
    {x}{\mathcal{L}_\mu(x, \lambda, \mu) = f(x) + \sum_{i \in \mathcal{I}} \lambda_i g_i(x) + \sum_{j \in \mathcal{E}} \mu_j h_j(x) + \frac{\mu}{2} \left( \sum_{i \in \mathcal{I}} [\max(0, g_i(x) + \frac{\lambda_i}{\mu})]^2 + \sum_{j \in \mathcal{E}} [h_j(x)]^2 \right)}{}{}
\end{mini*}

These methods typically:
\begin{itemize}
    \item Alternately minimize $\mathcal{L}_\mu$ with respect to $x$ and update $\lambda, \mu$
    \item Avoid the ill-conditioning associated with pure penalty methods
    \item Converge to the exact solution for finite values of $\mu$
\end{itemize}

\section{Sequential Quadratic Programming (SQP)}

SQP methods solve constrained optimization problems by generating a sequence of quadratic programming subproblems.

At each iteration, SQP solves:
\begin{mini*}
    {d \in \mathbb{R}^n}{\nabla f(x_k)^T d + \frac{1}{2} d^T B_k d}{}{}
    \addConstraint{\nabla g_i(x_k)^T d + g_i(x_k)}{\leq 0,\quad}{i \in \mathcal{I}}
    \addConstraint{\nabla h_j(x_k)^T d + h_j(x_k)}{= 0,\quad}{j \in \mathcal{E}}
\end{mini*}
where $B_k$ approximates the Hessian of the Lagrangian.

The solution $d_k$ provides a search direction, and a line search is performed to determine the step length:
\[
x_{k+1} = x_k + \alpha_k d_k
\]

\section{Merit Functions and the Maratos Effect}

In constrained optimization, merit functions are used to measure progress toward the solution. A common merit function is:
\[
\phi_\mu(x) = f(x) + \mu \left( \sum_{i \in \mathcal{I}} \max(0, g_i(x)) + \sum_{j \in \mathcal{E}} |h_j(x)| \right)
\]

The Maratos effect is a phenomenon where a good step that approaches the solution may be rejected because the merit function increases. Techniques to address this include:

\begin{itemize}
    \item Second-order corrections
    \item Non-monotone line searches
    \item Filter methods that separate progress in the objective and constraint violation
\end{itemize}

These approaches help maintain convergence properties while avoiding the pitfalls of strict merit function reductions at each step.
