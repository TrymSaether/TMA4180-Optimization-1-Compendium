
\chapter{Konveks optimering}

Hvis et optimeringsproblem er konvekst, kan vi være sikre på at vi finner en global optimal løsning.

\section*{Definisjoner}

\begin{definition}{Lineært optimeringsproblem}{}
  Et lineært optimeringsproblem er et optimeringsproblem på formen
  \begin{align*}
    \text{minimer}     & \quad \min_{x \in \R^n} f(x)                \\
    \text{betinget av} & \quad h_i(x) \leq 0, \quad i = 1, \ldots, m \\
                       & \quad g_j(x) = 0, \quad j = 1, \ldots, p
  \end{align*}
  hvor \(f, h_i, g_j\) er lineære funksjoner.
\end{definition}

\begin{example}{Lineær funksjon}{}
  La \(f(\symbf{x}) = c^T\symbf{x} + d\) være en lineær funksjon, hvor \(c\) er en vektor normal til en hyperplan og \(d\) er en konstant.
  Da er \(f(\symbf{x}) = 0\) en lineær likning som definerer en hyperplan i \(\R^n\).
\end{example}

\begin{example}{Lineær regresjon}{}
  La \(X \in \R^{n \times m}\) være en matrise med observasjoner og \(y \in \R^n\) være en vektor med målinger.
  Lineær regresjon er et eksempel på et lineært program hvor vi ønsker å finne en vektor \(w \in \R^m\) som minimerer kvadratfeilen
  \begin{equation*}
    \min_{w \in \R^m} \norm{Xw - y}_2^2.
  \end{equation*}
\end{example}
