\chapter{Ubundet Optimalisering}
\label{chap:unconstrained_optimization}

Ubundet optimalisering (eller fri optimalisering) refererer til problemer uten eksplisitte restriksjoner på variablene. 
Målet er å minimere en \textit{glatt objektfunksjon} \( f : \mathbb{R}^n \to \mathbb{R} \) over hele \(\mathbb{R}^n\):
\[
  \min_{\symbf{x} \in \mathbb{R}^n} f(\symbf{x}),
\]
hvor løsningen \(\symbf{x}^*\) tilfredsstiller
\[
  f(\symbf{x}^*) \;\le\; f(\symbf{x}) 
  \quad \forall \symbf{x} \in \mathbb{R}^n.
\]

\section{Grunnleggende Konsepter}

\subsection{Egenskaper}

\begin{itemize}
  \item \textbf{Ingen restriksjoner}: Den tillatte mengden (feasible set) er hele \(\mathbb{R}^n\), uten likhets- eller ulikhetsbetingelser.
  \item \textbf{Enklere oppsett}: Det er ikke behov for å håndtere restriksjoner i selve problemet.
  \item \textbf{Fokuserer kun på målfunksjonen}: Algoritmer søker etter punkter \(\symbf{x}\) som reduserer \(f(\symbf{x})\) direkte, uten å ta hensyn til andre forhold.
\end{itemize}

\subsection{Konvergens}
For å sikre konvergens til en stasjonær løsning \(\symbf{x}^*\), må følgende betingelser typisk oppfylles:
\begin{enumerate}
  \item \(f(\symbf{x})\) er kontinuerlig deriverbar (\(C^1\)).
  \item Gradienten \(\nabla f(\symbf{x})\) eksisterer og er Lipschitz-kontinuerlig.
  \item \hyperref[def:level_set]{Nivåsettene} \(\mathcal{L}_f(\alpha)\) er begrenset og lukkede \hyperref[def:compact_set]{(kompakte)}.
\end{enumerate}

Under disse antagelsene sikres konvergens til en stasjonær løsning \(\symbf{x}^*\) der 
\(\nabla f(\symbf{x}^*) = 0\). 
Dette innebærer at
\[
  \lim_{k \to \infty} \|\nabla f(\symbf{x}_k)\| = 0,
\]
hvor \(\symbf{x}_k\) er iteratene generert av en optimaliseringsalgoritme, og \(\symbf{x}^*\) tilfredsstiller de førsteordens nødvendige betingelsene \(\nabla f(\symbf{x}^*) = 0\).

\section{Linjesøk}

Linjesøk omfatter metoder for å finne en egnet skrittlengde \(\alpha\) som reduserer objektfunksjonen \(f(\symbf{x})\) i en gitt retning.

\subsection{Backtracking Line Search}
Backtracking Line Search er en metode for å finne en passende skrittlengde \(\alpha\) som sikrer en ønsket reduksjon i \(f(\symbf{x})\) langs en retning \(\symbf{d}\).

\subsubsection{Algoritme}
\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Startpunkt \( \symbf{x} \), retning \( \symbf{d} \), initial skrittlengde \( \alpha \), \textit{backtracking}-parameter \( \beta \in (0,1) \), \textit{skalering} \( \rho \in (0,1) \)}
  \KwOut{Skrittlengde \( \alpha \)}
  \While{\( f(\symbf{x} + \alpha \symbf{d}) > f(\symbf{x}) + \beta \,\alpha \,\nabla f(\symbf{x})^T \symbf{d} \)}{
    \(\alpha \leftarrow \rho \,\alpha\)
  }
  \caption{Backtracking Line Search}
\end{algorithm}

\subsection{Betingelser for Tilstrekkelig Reduksjon}

\subsubsection{Wolfe-betingelsene}
\begin{definition}{Wolfe-betingelsene}{wolfe_conditions}
  \begin{enumerate}
    \item \textbf{Armijo-betingelsen}:
          \[
            f(\symbf{x} + \alpha \symbf{d}) 
            \;\le\; 
            f(\symbf{x}) 
            \;+\; 
            \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d}.
          \]
    \item \textbf{Krav til avtagende rate}:
          \[
            \nabla f(\symbf{x} + \alpha \symbf{d})^T \symbf{d} 
            \;\ge\; 
            \rho \,\nabla f(\symbf{x})^T \symbf{d}.
          \]
  \end{enumerate}
\end{definition}

\subsubsection{Armijo-betingelsen}
\begin{definition}{Armijo-betingelsen}{armijo_condition}
  Armijo-betingelsen sikrer at steget gir en tilstrekkelig reduksjon i funksjonsverdien. Den krever at
  \[
    f(\symbf{x} + \alpha \symbf{d}) 
    \;\le\; 
    f(\symbf{x}) 
    \;+\; 
    \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d},
  \]
  hvor \(\beta\) er en liten positiv konstant. Dette tilsier at \(f\) reduseres i tråd med en lineær modell.
\end{definition}

\subsubsection{Goldstein-betingelsen}
\begin{definition}{Goldstein-betingelsen}{goldstein_condition}
  Goldstein-betingelsen krever at den nye funksjonsverdien etter steget ligger innenfor et avgrenset intervall:
  \[
    f(\symbf{x}) + (1-\beta)\,\alpha\,\nabla f(\symbf{x})^T \symbf{d} 
    \;\le\; 
    f(\symbf{x} + \alpha \symbf{d}) 
    \;\le\; 
    f(\symbf{x}) + \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d}.
  \]
  Dette forhindrer at vi tar et for lite eller for stort steg.
\end{definition}

\subsubsection{Kurvbetingelsen}
\begin{definition}{Curvature-betingelsen}{curvature_condition}
  Kurvbetingelsen ser på gradientens endring etter steget og krever
  \[
    \bigl|\nabla f(\symbf{x} + \alpha \symbf{d})^T \symbf{d}\bigr| 
    \;\le\; 
    -\sigma \,\nabla f(\symbf{x})^T \symbf{d},
  \]
  for en konstant \(\sigma \in (0,1)\). Dette sikrer at gradienten ikke endrer retning for brått, og at man unngår “overkorrigering”.
\end{definition}

\subsubsection{Strong Wolfe-betingelsene}
\begin{definition}{Strong Wolfe-betingelsene}{strong_wolfe_conditions}
  Strong Wolfe-betingelsene kombinerer Armijo-betingelsen med en kurvbetingelse:
  \begin{enumerate}
    \item Armijo-betingelsen:
          \[
            f(\symbf{x} + \alpha \symbf{d}) 
            \;\le\; 
            f(\symbf{x}) 
            \;+\; 
            \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d}.
          \]
    \item Kurvbetingelsen:
          \[
            \bigl|\nabla f(\symbf{x} + \alpha \symbf{d})^T \symbf{d}\bigr| 
            \;\le\; 
            -\sigma \,\nabla f(\symbf{x})^T \symbf{d}.
          \]
  \end{enumerate}
\end{definition}

\subsubsection{Goldstein-Wolfe-betingelsene}
\begin{definition}{Goldstein-Wolfe-betingelsene}{goldstein_wolfe_conditions}
  Goldstein-Wolfe-betingelsene krever at Armijo- og Goldstein-betingelsene begge er oppfylt:
  \begin{enumerate}
    \item Armijo-betingelsen:
          \[
            f(\symbf{x} + \alpha \symbf{d}) 
            \;\le\; 
            f(\symbf{x}) 
            \;+\; 
            \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d}.
          \]
    \item Goldstein-betingelsen:
          \[
            f(\symbf{x}) + (1-\beta)\,\alpha\,\nabla f(\symbf{x})^T \symbf{d} 
            \;\le\; 
            f(\symbf{x} + \alpha \symbf{d}) 
            \;\le\; 
            f(\symbf{x}) + \beta\,\alpha\,\nabla f(\symbf{x})^T \symbf{d}.
          \]
  \end{enumerate}
\end{definition}

\section{Newtons Metode}
\label{sec:newtons_method}

Newtons metode er en iterativ tilnærming for å finne lokale ekstremalpunkter i en funksjon. Den bruker andreordens informasjon (Hesse-matrisen) for å avlede en søkeretning som ofte gir raskere konvergens enn gradientbaserte metoder alene.

\subsection{Algoritme}
Gitt en startverdi \( \symbf{x}_0 \), genererer Newtons metode en sekvens av iterasjoner:
\[
  \symbf{x}_{k+1} 
  \;=\; 
  \symbf{x}_k 
  \;-\; 
  [\nabla^2 f(\symbf{x}_k)]^{-1} \,\nabla f(\symbf{x}_k),
\]
der:
\begin{itemize}
  \item \(\symbf{x}_{k+1}\) er neste iterasjon.
  \item \(\nabla f(\symbf{x}_k)\) er gradienten av \(f\) evaluert i \(\symbf{x}_k\).
  \item \(\nabla^2 f(\symbf{x}_k)\) er Hesse-matrisen av \(f\) i \(\symbf{x}_k\).
\end{itemize}

\paragraph{Søkeretning}
Søkeretningen finnes ved:
\[
  \symbf{d}_k 
  \;=\; 
  -[\nabla^2 f(\symbf{x}_k)]^{-1}\,\nabla f(\symbf{x}_k).
\]
\paragraph{Oppdatering}
\[
  \symbf{x}_{k+1} 
  \;=\; 
  \symbf{x}_k + \alpha_k \,\symbf{d}_k,
\]
\subparagraph{Skrittlengde}
Skrittlengden \(\alpha_k\) kan finnes ved linjesøk (f.eks.\ Backtracking Line Search).

\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Startpunkt \( \symbf{x}_0 \), toleranse \( \epsilon \), maks antall iterasjoner \( K \)}
  \KwOut{Omtrentelig løsning \( \symbf{x}^* \)}
  \For{\( k = 0, 1, 2, \ldots, K\)}{
    Beregn søkeretning: 
    \(\symbf{d}_k 
      = -[\nabla^2 f(\symbf{x}_k)]^{-1}\,\nabla f(\symbf{x}_k)\)\;
    Finn skrittlengde \(\alpha_k\) (f.eks.\ ved Backtracking Line Search)\;
    Oppdater \( 
      \symbf{x}_{k+1} 
      = \symbf{x}_k + \alpha_k \symbf{d}_k
    \)\;
    \If{\(\|\nabla f(\symbf{x}_{k+1})\| < \epsilon\)}{
      \Return \(\symbf{x}_{k+1}\)\;
    }
  }
  \caption{Newtons metode}
\end{algorithm}

\subsection{Konvergens av Newtons Metode}
Newtons metode kan konvergere lokalt kvadratisk under visse forutsetninger. La \( f : \mathbb{R}^n \to \mathbb{R} \) være to ganger kontinuerlig deriverbar. Anta at:

\begin{enumerate}
  \item Det finnes et \(\symbf{x}^*\) slik at \(\nabla f(\symbf{x}^*) = 0\).
  \item Hesse-matrisen \(\nabla^2 f(\symbf{x}^*)\) er regulær (invertibel).
  \item \(\nabla^2 f\) er Lipschitz-kontinuerlig i et nabolag rundt \(\symbf{x}^*\); dvs.\ det finnes \( L > 0 \) slik at
        \[
          \| \nabla^2 f(\symbf{x}) - \nabla^2 f(\symbf{y}) \| 
          \;\le\; 
          L\,\|\symbf{x} - \symbf{y}\|,
        \]
        for alle \(\symbf{x}, \symbf{y}\) i dette nabolaget.
\end{enumerate}

Da kan man vise at for en startverdi \(\symbf{x}_0\) nær \(\symbf{x}^*\), vil Newtons metode konvergere kvadratisk mot \(\symbf{x}^*\).

\begin{proof}{}{}
  La \(\symbf{e}_k = \symbf{x}_k - \symbf{x}^*\) være feilen ved iterasjon \(k\). 
  Ved Taylor-utvidelse av gradienten får vi:
  \[
    \nabla f(\symbf{x}_k) 
    = \nabla^2 f(\symbf{x}^*)\,(\symbf{x}_k - \symbf{x}^*) + r(\symbf{x}_k),
  \]
  hvor \(\|r(\symbf{x}_k)\|\) er \(\mathcal{O}(\|\symbf{e}_k\|^2)\). Newtons oppdateringsregel er
  \[
    \symbf{x}_{k+1} 
    = \symbf{x}_k 
    - [\nabla^2 f(\symbf{x}_k)]^{-1}\,\nabla f(\symbf{x}_k).
  \]
  I et lite nabolag av \(\symbf{x}^*\) kan vi anta at 
  \(\nabla^2 f(\symbf{x}_k)\approx \nabla^2 f(\symbf{x}^*)\), slik at
  \[
    \symbf{e}_{k+1} 
    \;=\; \symbf{x}_{k+1} - \symbf{x}^* 
    \;\approx\;
    -[\nabla^2 f(\symbf{x}^*)]^{-1}\,r(\symbf{x}_k).
  \]
  Dermed blir
  \[
    \|\symbf{e}_{k+1}\|
    \;\lesssim\;
    \|[\nabla^2 f(\symbf{x}^*)]^{-1}\|\,
    \|r(\symbf{x}_k)\|
    \;\le\;
    C \|\symbf{e}_k\|^2,
  \]
  for en konstant \(C>0\). Dette illustrerer \textit{kvadratisk konvergens} når \(\symbf{x}_0\) er nært \(\symbf{x}^*\).
\end{proof}

\subsection{Bevis for Kvadratisk Konvergens}
\label{sec:newton_quadratic_convergence}
Argumentet ovenfor kan formuleres mer detaljert ved å vise at feilen \(\|\symbf{e}_{k+1}\|\) er proporsjonal med \(\|\symbf{e}_k\|^2\). Dette er selve definisjonen av kvadratisk konvergens.

\subsection{Kommentarer}
Beviset forutsetter at startverdien \(\symbf{x}_0\) er tilstrekkelig nær \(\symbf{x}^*\). I praksis benyttes ofte modifikasjoner, som for eksempel \textit{dempet Newton} (med linjesøk), for å sikre global konvergens før man oppnår den raske, lokale konvergensfasen.

\subsection{Egenskaper}
\begin{itemize}
  \item \textbf{Kvadratisk konvergens}: Under gitte forutsetninger dobles antall riktige sifre omtrent for hver iterasjon.
  \item \textbf{Krever andreordens deriverbarhet}: \(\nabla^2 f(\symbf{x})\) må eksistere og være invertibel.
  \item \textbf{Beregning kan være kostbar}: Å beregne og invertere \(\nabla^2 f(\symbf{x})\) er dyrt, spesielt i store dimensjoner.
  \item \textbf{Global vs.\ lokal konvergens}: Metoden garanterer ikke nødvendigvis global konvergens uten videre tiltak.
\end{itemize}

\subsection{Modifikasjoner}
\begin{itemize}
  \item \textbf{Dempet Newtons metode}: Benytter en skrittlengde \(\alpha_k\) (linjesøk) for å unngå for store steg.
  \item \textbf{Kvasi-Newton-metoder}: Reduserer kostnader ved å approksimere Hesse-matrisen (f.eks.\ BFGS, DFP).
\end{itemize}


\section{Konjugerte Gradientmetode (CG)}
\label{sec:conjugate_gradient}

Konjugert Gradient (CG) metoden er en effektiv iterativ algoritme for å løse lineære ligningssystemer og optimere kvadratiske funksjoner. Den er spesielt nyttig for store, men sparse systemer.

\subsection{Grunnleggende Konsept}
CG-metoden genererer en sekvens av retninger som er konjugerte med hensyn til en symmetrisk, positiv definit matrise. For en kvadratisk funksjon:
\[
  f(\symbf{x}) = \frac{1}{2}\symbf{x}^T\symbf{A}\symbf{x} - \symbf{b}^T\symbf{x} + c,
\]
hvor \(\symbf{A}\) er symmetrisk, positiv definit, søker CG-metoden å løse \(\nabla f(\symbf{x}) = \symbf{A}\symbf{x} - \symbf{b} = \symbf{0}\).

\subsection{Algoritme for Kvadratiske Funksjoner}

\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Startpunkt \(\symbf{x}_0\), toleranse \(\epsilon > 0\)}
  \KwOut{Omtrentelig løsning \(\symbf{x}^*\)}
  \(\symbf{r}_0 \gets \symbf{b} - \symbf{A}\symbf{x}_0\) \tcc*{Residual}
  \(\symbf{p}_0 \gets \symbf{r}_0\) \tcc*{Første søkeretning}
  \For{\(k = 0, 1, 2, \ldots\)}{
    \If{\(\|\symbf{r}_k\| < \epsilon\)}{
      \Return \(\symbf{x}_k\) \tcc*{Konvergens oppnådd}
    }
    \(\alpha_k \gets \frac{\symbf{r}_k^T\symbf{r}_k}{\symbf{p}_k^T\symbf{A}\symbf{p}_k}\) \tcc*{Optimal skrittlengde}
    \(\symbf{x}_{k+1} \gets \symbf{x}_k + \alpha_k\symbf{p}_k\) \tcc*{Oppdater løsning}
    \(\symbf{r}_{k+1} \gets \symbf{r}_k - \alpha_k\symbf{A}\symbf{p}_k\) \tcc*{Oppdater residual}
    \(\beta_{k+1} \gets \frac{\symbf{r}_{k+1}^T\symbf{r}_{k+1}}{\symbf{r}_k^T\symbf{r}_k}\) \tcc*{Konjugasjonsparameter}
    \(\symbf{p}_{k+1} \gets \symbf{r}_{k+1} + \beta_{k+1}\symbf{p}_k\) \tcc*{Ny søkeretning}
  }
  \caption{Konjugert Gradient Metode (kvadratisk tilfelle)}
\end{algorithm}

\subsection{Ikke-lineær Konjugert Gradient Metode}
For generelle ikke-lineære funksjoner tilpasses algoritmen:

\begin{algorithm}[H]
  \SetAlgoLined
  \KwIn{Startpunkt \(\symbf{x}_0\), toleranse \(\epsilon > 0\)}
  \KwOut{Omtrentelig løsning \(\symbf{x}^*\)}
  \(\symbf{g}_0 \gets \nabla f(\symbf{x}_0)\) \tcc*{Gradient}
  \(\symbf{p}_0 \gets -\symbf{g}_0\) \tcc*{Første søkeretning}
  \For{\(k = 0, 1, 2, \ldots\)}{
    \If{\(\|\symbf{g}_k\| < \epsilon\)}{
      \Return \(\symbf{x}_k\) \tcc*{Konvergens oppnådd}
    }
    Finn \(\alpha_k\) ved linjesøk som minimerer \(f(\symbf{x}_k + \alpha\symbf{p}_k)\) \;
    \(\symbf{x}_{k+1} \gets \symbf{x}_k + \alpha_k\symbf{p}_k\) \tcc*{Oppdater løsning}
    \(\symbf{g}_{k+1} \gets \nabla f(\symbf{x}_{k+1})\) \tcc*{Ny gradient}
    Beregn \(\beta_{k+1}\) (f.eks. Fletcher-Reeves eller Polak-Ribière)\;
    \(\symbf{p}_{k+1} \gets -\symbf{g}_{k+1} + \beta_{k+1}\symbf{p}_k\) \tcc*{Ny søkeretning}
  }
  \caption{Ikke-lineær Konjugert Gradient Metode}
\end{algorithm}

\subsection{Valg av beta-parameter}

Flere formler eksisterer for \(\beta\) i ikke-lineære CG-metoder:
\begin{itemize}
  \item \textbf{Fletcher-Reeves}: \(\beta_{k+1}^{FR} = \frac{\symbf{g}_{k+1}^T\symbf{g}_{k+1}}{\symbf{g}_k^T\symbf{g}_k}\)
  \item \textbf{Polak-Ribière}: \(\beta_{k+1}^{PR} = \frac{\symbf{g}_{k+1}^T(\symbf{g}_{k+1}-\symbf{g}_k)}{\symbf{g}_k^T\symbf{g}_k}\)
  \item \textbf{Hestenes-Stiefel}: \(\beta_{k+1}^{HS} = \frac{\symbf{g}_{k+1}^T(\symbf{g}_{k+1}-\symbf{g}_k)}{\symbf{p}_k^T(\symbf{g}_{k+1}-\symbf{g}_k)}\)
\end{itemize}

\subsection{Egenskaper}
\begin{itemize}
  \item \textbf{Hurtig konvergens}: For kvadratiske funksjoner konvergerer metoden på maksimalt \(n\) iterasjoner (i eksakt aritmetikk), hvor \(n\) er problemdimensjon.
  \item \textbf{Minneeffektivitet}: Lagrer kun et begrenset antall vektorer, noe som gjør den egnet for store problemer.
  \item \textbf{Numerisk stabilitet}: God oppførsel under avrundingsfeil, spesielt med periodisk omstart.
  \item \textbf{Forbehandling}: Kan akselereres videre ved å bruke en prekondisjonerer som transformerer problemet.
\end{itemize}

\subsection{Prekondisjonering}
Konvergenshastigheten kan forbedres ved å bruke en prekondisjonerer \(\symbf{M}\) som approksimerer \(\symbf{A}\) på en inverterbar måte. Den prekondisjonerte algoritmen løser det transformerte systemet:
\[
  \symbf{M}^{-1}\symbf{A}\symbf{x} = \symbf{M}^{-1}\symbf{b}.
\]
Dette reduserer kondisjonsnummeret og akselererer konvergensen betydelig i mange praktiske tilfeller.